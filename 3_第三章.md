# 3.3
对数几率回归，虽然名字含回归，但其解决的是分类问题。  
对数几率函数（logistic function）：$y=\frac{1}{1+e^{-z}}$。为Sigmoid函数（形如S的函数）的一种。它可以用来表示概率。  
将其和线性回归结合，可得到$y=\frac{1}{1+e^{-(w^Tx+b)}}$，可转化为$ln(\frac{y}{1-y})=w^Tx+b$。$\frac{y}{1-y}$称为几率，$y$是样本$x$作为正例的可能性，$1-y$是其反例的可能性，两者的比值即称为几率。对几率取对数便得到了对数几率$ln\frac{y}{1-y}$。  
因此，回归式子$y'=w^Tx+b$，实际是回归对数几率，即对数几率回归。  
注意，回归出来的函数$f(x)=w^Tx+b$，代入$x$后，若大于零，说明对数几率大于零，说明几率大于一，说明是正例的可能性比反例的可能性大，因此判为正例。而若想得到为正例的概率，需计算$\frac{1}{1+e^{-f(x)}}$。  
下面推导如何求参数$w$和$b$，由于$y$的含义是为正例的概率，且$y=\frac{1}{1+e^{-(w^Tx+b)}}$，那么为反例的概率为$1-y=\frac{1}{1+e^{w^Tx+b}}$，我们可以用极大似然法来解决该问题。（概率分布形式已知，但参数未知，考虑极大似然法或者贝叶斯法。）   
极大似然法是让每个样本属于其真实标记的概率越大越好，因此我们要最大化对数似然$l(w,b)=\sum_{i=1}^mlnp(y_i|x_i;w,b)$。化简后可得到一个关于$w$和$b$的凸函数，利用梯度下降法和牛顿法等都可求得最优解。  
代码见3_3.py，此处是用数值解来求导数，一般还是推导一下较好。
# 3.4
# 3.5
线性判别分析，可以看作一种降维的方式。在维数过多，维与维之间依赖性很强的情况下可以使用。它将数据降成一维，并尽力保证维与维之间离得尽可能远，相同维内的样本离得尽可能近。  

